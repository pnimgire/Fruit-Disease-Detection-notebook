{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"cells":[{"cell_type":"code","metadata":{"id":"g_jUEI3jjtJI"},"source":["import numpy as np\n","import PIL\n","import pandas as pd\n","import tensorflow as tf\n","from scipy.io import loadmat, savemat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjxaJIHolGij"},"source":["#Load images and convert to numpy arrays\n","train_annotations = np.array(pd.read_csv('Train.csv'))\n","img_ids = train_annotations[:, 0].astype(np.str)\n","img_labels = train_annotations[:, 1].astype(np.str)\n","img_directory = 'Train_Images/'\n","classdict = {'fruit_brownspot':0, 'fruit_healthy':1, 'fruit_woodiness':2}\n","unique_ids = np.unique(img_ids)\n","\n","nimages = unique_ids.shape[0]\n","All_imgs = np.zeros([nimages, 256, 256, 3]).astype(np.float32)\n","All_target = np.zeros(nimages)\n","for i in range(nimages):\n","    if i%100 == 0:\n","        pdone = i/nimages*100\n","        print('Percentage done: %.1f' % pdone)\n","    fname = img_directory + unique_ids[i] + '.jpg'\n","    img = PIL.Image.open(fname).resize([256, 256])\n","    All_imgs[i, :, :, :] = np.array(img)/255\n","    label_ind = np.where(img_ids == unique_ids[i])[0][0]\n","    All_target[i] = classdict[img_labels[label_ind]]\n","\n","outdict = {'All_imgs':All_imgs, 'All_target':All_target}\n","savemat('full_set.mat', outdict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JdLPfRb2lUa2"},"source":["#Split images into training and validation sets\n","class1_inds = np.where(All_target == 0)[0]\n","class2_inds = np.where(All_target == 1)[0]\n","class3_inds = np.where(All_target == 2)[0]\n","\n","np.random.seed(123)\n","np.random.shuffle(class1_inds)\n","np.random.shuffle(class2_inds)\n","np.random.shuffle(class3_inds)\n","\n","class1_intrain = int(len(class1_inds)*0.85)\n","class2_intrain = int(len(class2_inds)*0.85)\n","class3_intrain = int(len(class3_inds)*0.85)\n","\n","train_inds = np.append(class1_inds[:class1_intrain], \n","                       class2_inds[:class2_intrain])\n","train_inds = np.append(train_inds,\n","                       class3_inds[:class3_intrain])\n","np.random.shuffle(train_inds)\n","\n","train_imgs = All_imgs[train_inds, :, :, :]\n","train_target = All_target[train_inds]\n","\n","val_imgs = np.delete(All_imgs, train_inds, axis=0)\n","val_target = np.delete(All_target, train_inds, axis=0)\n","\n","outdict = {'train_imgs':train_imgs,\n","           'train_target':train_target,\n","           'val_imgs':val_imgs,\n","           'val_target':val_target}\n","savemat('split_set.mat', outdict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sdVCHyTD_ERQ"},"source":[""],"execution_count":null,"outputs":[]}]}